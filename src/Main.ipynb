{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.9.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp = np.copy(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = np.copy(supp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'total_rewards' referenced before assignment",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-980a3e7c0eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     '''\n\u001b[1;32m     67\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-980a3e7c0eca>\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'total_rewards' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from core.DQNAgent import DQNAgent\n",
    "from core.CartPoleNeuralNetwork import CartPoleNeuralNetwork\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import Video\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from CartPoleWrapper import CartPoleWrapper\n",
    "import sys\n",
    "\n",
    "\n",
    "import pyvirtualdisplay\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()\n",
    "\n",
    "def plot_videos(videos_path='.', output_file_path='.'):\n",
    "  stringa = 'ffmpeg -i \\\"concat:'\n",
    "  elenco_video = glob.glob(f'{videos_path}/*.mp4')\n",
    "  if len(elenco_video) == 0:\n",
    "      print('0 mp4 found in this path')\n",
    "      return\n",
    "  elenco_file_temp = []\n",
    "  for f in elenco_video:\n",
    "    file = videos_path + '/temp' + str(elenco_video.index(f) + 1) + '.ts'\n",
    "    os.system('ffmpeg -i ' + f + ' -c copy -bsf:v h264_mp4toannexb -f mpegts ' + file)\n",
    "    elenco_file_temp.append(file)\n",
    "  for f in elenco_file_temp:\n",
    "    stringa += f\n",
    "    if elenco_file_temp.index(f) != len(elenco_file_temp)-1:\n",
    "      stringa += '|'\n",
    "    else:\n",
    "      stringa += f'\\\" -c copy  -bsf:a aac_adtstoasc {output_file_path}'\n",
    "  os.system(stringa)\n",
    "  display(Video(output_file_path, embed=True))\n",
    "\n",
    "def plot_metrics():\n",
    "    cycol = cycle('bgrcmk')\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "    samples = 20\n",
    "    total_rewards.append([np.array(total_rewards[-(total_rewards%samples):]).mean()] * (total_rewards/samples - total_rewards%samples))\n",
    "    total_rewards = np.array(total_rewards).reshape(20, -1).mean(axis=1)\n",
    "\n",
    "    episodes = []\n",
    "    for i in range(n_episodes):\n",
    "        if i % (n_episodes / len(total_rewards)) == 0:\n",
    "            episodes.append(i)\n",
    "    \n",
    "    \n",
    "    ax1.set_xlabel('episodes')\n",
    "    ax1.set_ylabel('total_rewards')\n",
    "    ax1.plot(episodes, total_rewards, c=next(cycol))\n",
    "    '''\n",
    "    ax2.set_xlabel('episodes')\n",
    "    ax2.set_ylabel('number_steps')\n",
    "    ax2.plot(episodes, number_steps, c=next(cycol))\n",
    "\n",
    "    ax3.set_xlabel('episodes')\n",
    "    ax3.set_ylabel('cost_function_means')\n",
    "    ax3.plot(episodes, cost_means, c=next(cycol))\n",
    "    '''\n",
    "    f.tight_layout()\n",
    "plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "agent = DQNAgent(env=CartPoleWrapper(gym.make(\"CartPole-v1\")), nn=CartPoleNeuralNetwork())\n",
    "\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "total_rewards = []\n",
    "number_steps = []\n",
    "cost_means = []\n",
    "n_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 154.26it/s]\n",
      "n_episodes_training: 11\tsteps: 13\ttotal_reward: 11.0\tmean_cost_function:0.6580767237800025\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 161.41it/s]\n",
      "n_episodes_training: 22\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5284366969845323\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 260.28it/s]\n",
      "n_episodes_training: 33\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.535577931353897\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 182.77it/s]\n",
      "n_episodes_training: 44\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6381844891608434\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 242.86it/s]\n",
      "n_episodes_training: 55\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5386842904109761\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 165.18it/s]\n",
      "n_episodes_training: 66\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5330615548956976\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 118.40it/s]\n",
      "n_episodes_training: 77\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5419891205551928\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 174.39it/s]\n",
      "n_episodes_training: 88\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.5469195728317985\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 119.45it/s]\n",
      "n_episodes_training: 99\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.5977425971842906\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 191.37it/s]\n",
      "n_episodes_training: 110\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.6666049054985346\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 157.43it/s]\n",
      "n_episodes_training: 121\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5446790147392895\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 139.62it/s]\n",
      "n_episodes_training: 132\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5450268645122622\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 142.71it/s]\n",
      "n_episodes_training: 143\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.5617771493228849\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 213.90it/s]\n",
      "n_episodes_training: 154\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5578302660178286\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 137.59it/s]\n",
      "n_episodes_training: 165\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5588947254696538\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 117.59it/s]\n",
      "n_episodes_training: 176\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.6281612238089376\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 197.28it/s]\n",
      "n_episodes_training: 187\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5548818128269988\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 229.94it/s]\n",
      "n_episodes_training: 198\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5676536444530279\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 187.27it/s]\n",
      "n_episodes_training: 209\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5692358833675398\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 209.21it/s]\n",
      "n_episodes_training: 220\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6399324413560978\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 130.91it/s]\n",
      "n_episodes_training: 231\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5731544039004963\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 265.69it/s]\n",
      "n_episodes_training: 242\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5767272404340416\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 186.46it/s]\n",
      "n_episodes_training: 253\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5766536171531932\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 193.78it/s]\n",
      "n_episodes_training: 264\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.5887866784237813\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 236.80it/s]\n",
      "n_episodes_training: 275\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5826585541410145\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 165.68it/s]\n",
      "n_episodes_training: 286\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5728815587418671\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 216.51it/s]\n",
      "n_episodes_training: 297\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.6100294676877673\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 158.60it/s]\n",
      "n_episodes_training: 308\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5766819083856147\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 204.73it/s]\n",
      "n_episodes_training: 319\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5785304142292446\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 181.06it/s]\n",
      "n_episodes_training: 330\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.5939439948966674\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 156.19it/s]\n",
      "n_episodes_training: 341\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.624003322734615\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 185.55it/s]\n",
      "n_episodes_training: 352\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6387626710620373\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 105.97it/s]\n",
      "n_episodes_training: 363\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.635731269952366\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 214.27it/s]\n",
      "n_episodes_training: 374\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5911324253625823\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 208.44it/s]\n",
      "n_episodes_training: 385\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5932265747933718\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 145.00it/s]\n",
      "n_episodes_training: 396\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.610725336362119\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 151.86it/s]\n",
      "n_episodes_training: 407\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.6581526368935573\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 195.65it/s]\n",
      "n_episodes_training: 418\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6016258406437858\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 188.71it/s]\n",
      "n_episodes_training: 429\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.6671646881171235\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 222.23it/s]\n",
      "n_episodes_training: 440\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.6237027435706146\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 262.04it/s]\n",
      "n_episodes_training: 451\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.6427574880441536\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 184.86it/s]\n",
      "n_episodes_training: 462\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.6481601487804171\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 273.23it/s]\n",
      "n_episodes_training: 473\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6145357849970012\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 185.35it/s]\n",
      "n_episodes_training: 484\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.6354246378934434\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 194.59it/s]\n",
      "n_episodes_training: 495\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6191273424525143\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 178.94it/s]\n",
      "n_episodes_training: 506\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6221773508271096\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 210.87it/s]\n",
      "n_episodes_training: 517\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7064702873185875\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 232.44it/s]\n",
      "n_episodes_training: 528\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6280868359127967\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 215.31it/s]\n",
      "n_episodes_training: 539\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.6507223557315258\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 175.11it/s]\n",
      "n_episodes_training: 550\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.6540337832873567\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 229.69it/s]\n",
      "n_episodes_training: 561\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6349418834866789\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 150.35it/s]\n",
      "n_episodes_training: 572\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7105966663844328\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 188.60it/s]\n",
      "n_episodes_training: 583\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.6806228942756323\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 163.83it/s]\n",
      "n_episodes_training: 594\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.699028172724268\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 233.35it/s]\n",
      "n_episodes_training: 605\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.648996641506186\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 221.67it/s]\n",
      "n_episodes_training: 616\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7164900608995779\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 230.36it/s]\n",
      "n_episodes_training: 627\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6566434001054324\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 115.20it/s]\n",
      "n_episodes_training: 638\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.5685425519023053\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 159.37it/s]\n",
      "n_episodes_training: 649\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.723581209487429\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 206.91it/s]\n",
      "n_episodes_training: 660\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.6670490455110712\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 186.43it/s]\n",
      "n_episodes_training: 671\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7565455543152036\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 221.16it/s]\n",
      "n_episodes_training: 682\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7382931671116323\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 162.29it/s]\n",
      "n_episodes_training: 693\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.6748498874198073\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 169.30it/s]\n",
      "n_episodes_training: 704\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7358922574732703\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 190.74it/s]\n",
      "n_episodes_training: 715\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7161227054745389\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 144.94it/s]\n",
      "n_episodes_training: 726\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.5733365753889239\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 198.11it/s]\n",
      "n_episodes_training: 737\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.7438680409021847\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 232.77it/s]\n",
      "n_episodes_training: 748\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7332637148603527\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 171.62it/s]\n",
      "n_episodes_training: 759\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7408087736711934\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 221.98it/s]\n",
      "n_episodes_training: 770\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7687115033545353\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 220.84it/s]\n",
      "n_episodes_training: 781\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7136201283824748\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 220.56it/s]\n",
      "n_episodes_training: 792\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7200025966469383\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 209.32it/s]\n",
      "n_episodes_training: 803\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7210080195197829\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 160.48it/s]\n",
      "n_episodes_training: 814\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7634345194849084\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 252.78it/s]\n",
      "n_episodes_training: 825\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.77110972404344\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 231.00it/s]\n",
      "n_episodes_training: 836\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7769634636870867\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 283.56it/s]\n",
      "n_episodes_training: 847\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8371684832081719\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 171.75it/s]\n",
      "n_episodes_training: 858\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:0.8310530439324744\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 298.65it/s]\n",
      "n_episodes_training: 869\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.7997213497343484\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 199.43it/s]\n",
      "n_episodes_training: 880\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.7977893089658323\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 213.38it/s]\n",
      "n_episodes_training: 891\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8053342912202115\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 191.95it/s]\n",
      "n_episodes_training: 902\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.6685730209403009\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 216.85it/s]\n",
      "n_episodes_training: 913\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.8158422833268523\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 211.37it/s]\n",
      "n_episodes_training: 924\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.8222858451896975\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 157.58it/s]\n",
      "n_episodes_training: 935\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8375091987706915\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 216.08it/s]\n",
      "n_episodes_training: 946\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.6501793822370487\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 109.51it/s]\n",
      "n_episodes_training: 957\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.8190519661038017\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 210.03it/s]\n",
      "n_episodes_training: 968\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.8648645139065523\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 132.05it/s]\n",
      "n_episodes_training: 979\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8271513375470331\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 247.05it/s]\n",
      "n_episodes_training: 990\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8365106502672679\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 243.64it/s]\n",
      "n_episodes_training: 1001\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8451725432482281\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 241.24it/s]\n",
      "n_episodes_training: 1012\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.9669383431209769\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 216.72it/s]\n",
      "n_episodes_training: 1023\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.9144822400378091\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 162.12it/s]\n",
      "n_episodes_training: 1034\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:0.9233889428390604\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 164.16it/s]\n",
      "n_episodes_training: 1045\tsteps: 13\ttotal_reward: 11.0\tmean_cost_function:0.8343822544199855\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 203.02it/s]\n",
      "n_episodes_training: 1056\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:0.8935671161464093\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 238.69it/s]\n",
      "n_episodes_training: 1067\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:1.0324856120243806\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 194.02it/s]\n",
      "n_episodes_training: 1078\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.8067180048460648\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 235.06it/s]\n",
      "n_episodes_training: 1089\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.9139468531424104\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 196.98it/s]\n",
      "n_episodes_training: 1100\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:0.9213339604997212\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 159.60it/s]\n",
      "n_episodes_training: 1111\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.0656153166152573\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 116.51it/s]\n",
      "n_episodes_training: 1122\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.0048244893112237\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 67.44it/s]\n",
      "n_episodes_training: 1133\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:1.0018198417564006\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 193.63it/s]\n",
      "n_episodes_training: 1144\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:1.0269521011475424\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 186.39it/s]\n",
      "n_episodes_training: 1155\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.0717220816421027\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 194.29it/s]\n",
      "n_episodes_training: 1166\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:0.9458373272219355\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 267.97it/s]\n",
      "n_episodes_training: 1177\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.0677431886345659\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 158.38it/s]\n",
      "n_episodes_training: 1188\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.1181866257838746\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 161.32it/s]\n",
      "n_episodes_training: 1199\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.065770219211793\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 180.59it/s]\n",
      "n_episodes_training: 1210\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.0809912520884128\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 159.67it/s]\n",
      "n_episodes_training: 1221\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.0987751733091358\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 137.36it/s]\n",
      "n_episodes_training: 1232\tsteps: 11\ttotal_reward: 9.0\tmean_cost_function:1.0567457696391285\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 209.71it/s]\n",
      "n_episodes_training: 1243\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.229633699754102\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 188.49it/s]\n",
      "n_episodes_training: 1254\tsteps: 12\ttotal_reward: 10.0\tmean_cost_function:1.0640125785645227\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 201.85it/s]\n",
      "n_episodes_training: 1265\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.2845565683536444\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 268.54it/s]\n",
      "n_episodes_training: 1276\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:1.4178553639317826\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 159.61it/s]\n",
      "n_episodes_training: 1287\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.234625961745563\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 183.30it/s]\n",
      "n_episodes_training: 1298\tsteps: 8\ttotal_reward: 6.0\tmean_cost_function:1.457774965022932\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 147.51it/s]\n",
      "n_episodes_training: 1309\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.271788573405684\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 218.15it/s]\n",
      "n_episodes_training: 1320\tsteps: 14\ttotal_reward: 12.0\tmean_cost_function:1.092386909237818\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 208.43it/s]\n",
      "n_episodes_training: 1331\tsteps: 13\ttotal_reward: 11.0\tmean_cost_function:1.1312949913097914\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 195.73it/s]\n",
      "n_episodes_training: 1342\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.4535022929850239\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 155.79it/s]\n",
      "n_episodes_training: 1353\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.3785296567577574\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 229.22it/s]\n",
      "n_episodes_training: 1364\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.5494930728129437\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 139.09it/s]\n",
      "n_episodes_training: 1375\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.5526112286622242\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 150.95it/s]\n",
      "n_episodes_training: 1386\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.6267868222598412\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 228.37it/s]\n",
      "n_episodes_training: 1397\tsteps: 10\ttotal_reward: 8.0\tmean_cost_function:1.532942089141942\n",
      "\n",
      "learning...: 100%|██████████| 10/10 [00:00<00:00, 195.67it/s]\n",
      "n_episodes_training: 1408\tsteps: 9\ttotal_reward: 7.0\tmean_cost_function:1.6822851142359108\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bd531f62e39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnumber_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcost_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_cost_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'learning...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, write_bytes, lock_args, nrows, colour, delay, gui, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;31m# Init the time counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36mdisp_len\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mANSI\u001b[0m \u001b[0mcontrol\u001b[0m \u001b[0mcodes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwide\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_text_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRE_ANSI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36m_text_width\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_text_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meast_asian_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'FW'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_text_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meast_asian_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'FW'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while n_episodes < 10000:\n",
    "    total_reward, steps, mean_cost_function = agent.start_episode_and_evaluate(DISCOUNT_FACTOR, LEARNING_RATE, 0, render=False, optimize=False)\n",
    "    print(f'\\nn_episodes_training: {n_episodes}\\tsteps: {steps}\\ttotal_reward: {total_reward}\\tmean_cost_function:{mean_cost_function}', flush = True)\n",
    "    total_rewards.append(total_reward)\n",
    "    number_steps.append(steps)\n",
    "    cost_means.append(mean_cost_function)\n",
    "\n",
    "    for i in tqdm(range(10), 'learning...'):\n",
    "        agent.start_episode(DISCOUNT_FACTOR, LEARNING_RATE, 1)\n",
    "    n_episodes += i+1\n",
    "\n",
    "    if n_episodes % 110 == 0:\n",
    "        agent.save(f'saves/data{n_episodes}.nn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "agent.env = gym.wrappers.Monitor(agent.env, 'recording/tmp-videos', force=True, video_callable=lambda episode_id: True)\n",
    "agent.nn.load('saves/buona.nn')\n",
    "\n",
    "for i in range(20):\n",
    "    total_reward, steps, mean_cost_function = agent.start_episode_and_evaluate(DISCOUNT_FACTOR, LEARNING_RATE, 0, render=True, optimize=False)\n",
    "    print(f'{i}\\t{steps}\\t{total_reward}\\t{mean_cost_function}')\n",
    "env.close()\n",
    "\n",
    "agent.env = agent.env.env\n",
    "plot_videos('recording/tmp-videos', 'recording/0-episodes.mp4')"
   ]
  },
  {
   "source": [
    "evaluation\n",
    "\n",
    "train 1 000 (with %10 evaluation)\n",
    "\n",
    "evaluation\n",
    "\n",
    "train 9 000 - 10 000 (with %90 evaluation)\n",
    "\n",
    "evaluation\n",
    "\n",
    "train 90 000 - 100 000\n",
    "\n",
    "evaluation\n",
    "\n",
    "train 900 000 - 1 000 0000\n",
    "\n",
    "evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}